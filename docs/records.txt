
I would like to explore row types and extensible records.

The paper "Extensible Records with Scoped Labels" paper (leijen) presents a
type system, but expects to be used with HM-style type inference.

I would like to explore what a fully-explicit typed IR would look like.
(There may be some syntactic differences, because the original paper expects
type inference for assorted mixfix syntaxes)


Big idea: records (more generally, rows) can contain duplicate labels.
Operations usually act on the first (leftmost) occurrence of a label. Record
types are isomorphic when distinct labels can be permuted to make them equal,
but duplicate labels cannot have their order exchanged.

basic operations:
* kind Row
* type {} : Row
* type t : Type, rr : Row |- {l : t | rr} : Row
* empty record: {} : Record {}
* extension: v : t, r : Record rr |- { l = v, r } : Record {l : t | rr}
* projection: r : Record {l : t | rr} |- r.l : t
* restriction: r : Record {l : t | rr} |- r - l : Record rr

pros:
* minimal impact on typechecker
* simple typing rules
* natural integration with row polymorphism

cons:
* restriction operator is common, particularly as part of the derived "field
  update" operation. May need to do O(n) copies often
* runtime representation unclear
* equality of types is more complex than syntactic
  (In particular, consider coercion. If v : A and A == B, then v : B. However,
  for exchanging labels, this might not be a no-op.)



# Runtime Representation of Record Values:

I would also like to explore runtime representations for record values.
Ideally, I would be able to have { field1 : type, field2 : type2, ... } be
represented with

struct ????? {
  struct alloc_header header;
  type1 field1;
  type2 field2;
  ...
};

because that would let field projections be just struct field access.
(Efficient and type-safe) However, I would need to come up with a name for the
record, and deduplicate occurrences of the record type, which is not terribly
convenient.

Also, unboxed and non-pointer-sized fields work out in the obvious manner.
However, this approach doesn't really generalize to row-polymorphic records.


A second approach would be to bring back the "generic product" value representation:

struct generic_product {
  struct alloc_header header;
  uint32_t num_fields;
  struct alloc_header *fields[];
};

With this, all records are represented by a single type. Field projection use
type information at the call site to figure out what offset into 'fields' to
use. (In the same manner, the compiler can figure out how to cast the extracted
field.) However, this cannot easily support unboxed fields. (I don't even *have*
unboxed values yet, but I might want some eventually.)
Note: O(cheap) field access, O(n) extension/restriction


A third approach would be to use a hash table, mapping field names to values.
Similar to 'generic_product', this can only contain boxed values, and the info
from the projection site is used to correctly downcast the value.
I could probably figure out how to do minimal-overhead hash tables that exploit
the fixed number of fields and field names, but in the limit, it's probably the
same as 'generic_product'


Leijen's paper seems to promote a variant of the 'generic_product' approach,
where each field stores a field label and the value, sorted according to some
order on labels. If rows are closed, field offsets can be computed statically.
If rows are open, field offsets cannot be computed statically, but
worker-wrapper transformation can be used to push field offset computations
closer to call sites, where there's more type information.



# Mock-up: sorted span of key-value pairs

```
struct record_field {
  // some string-like type for field labels goes here
  struct label key;
  struct alloc_header *val;
};
struct record {
  struct alloc_header header;
  uint32_t num_fields;
  struct record_field fields[];
};

struct record *allocate_record(uint32_t num_fields, struct record_field *fields) {
  // malloc(header + u32 + size*record_field)
  // assume fields to be sorted?
  // copy fields into record
}

struct alloc_header *project_field(struct record *rec, struct label *l) {
  // binary search for index -- need total order on labels. (compare, leq, or lt)
  ;
  // Project the field value at that index.
}

struct record *extend_record(struct record *rec, struct label *l, struct alloc_header *v) {
  // bsearch to find point of insertion
  // memcpy items before insert point
  // append new value
  // memcpy items after insert point
  ;
}
```

Idea: labels are interned strings. bounded number of labels in a program,
immutable, only need total order.

```
struct label_pool {
  // some sort of interning scheme. Probably buffer & hash table (string -> offset).
};
private buffer interned_strings; // some internal representation. probably global.
struct label {
  // private details, unique identifier of entry in label pool.
}

// Total order on labels.
bool cmp_label_lt(const struct label *l1, const struct label *l2);

void init_labels_pool(void); // create the buffer
void destroy_all_labels(void); // free the buffer
struct label *make_label(size_t len, const char *s); // allocate/dedup label in buffer.
```


```
struct record_field {
  struct label l;
  uint32_t depth;
  struct alloc_header *val;
};
```

A record field consists of a label, how deeply shadowed the label is, and the
value. The shadowing level means that when binary search finds a label with
equal value, we can jump to the topmost value with that label.

```
def lookup_field(rec, l, lo, hi):
  // perform binary search over rec.fields[lo..hi] (inclusive-exclusive) for label 'l'.
  while lo < hi: // if lo == hi, zero-length slice: fail.
    let mid = lo + (hi - lo)/2;
    let field = &rec.fields[mid]
    if (l < field.l):
      hi = mid
      continue
    else if (field.l < l):
      lo = mid
      continue
    else: // total order implies field.l == l
      // Because shadowed labels are all equal, we need to find the outermost
      // label with this value. To solve this, each field contains the shadowing
      // depth of its label. For example, if binary search terminates at the
      // third copy of label 'foo', '.depth' will be '2'. If it by chance did
      // find the first copy, '.depth' would be '0'. Problem solved.
      return mid - fields[mid].depth

  panic!("unreachable: field projections are well-typed")

def project_field(rec, l):
  let idx = lookup_field(rec, l, 0, rec.num_fields)
  return rec.fields[idx].val
```




Record types with type fields: Nope.
====================================

I have considered merging existentials and record types, similar to how
closures have both value and type arguments.

This would let me write things like

```
type closure = { type envTy : *, env : envTy, code : code(envTy, argTys...) }
```

which could be compiled as

```
struct closure {
  // envTy is erased
  struct alloc_header *env; // env is uniform representation b/c value is polymorphic
  void (*code)(struct alloc_header *env, argTy1, ... argTyN); // standard code pointer
};
```

However, upon reflection, I do not think this is a good idea, for multiple reasons:

* The grammar of types now may include field projections of type components from record types:
  if `r : { type t : *, v : t }`, then `r.v : r.t`.
  This is uncomfortably close to dependent types, even if my late-stage IRs
  probably have enough structure to avoid full generality.
  (It's also uncomfortably close to ML modules, which are a front-end feature,
  not a "final stages of compilation" feature)

* The possibility of projecting existential field types also raises questions
  about validity. Is `{ type t : *, l : t }.t` a valid type?
  Probably not! There's that whole thing about escaping skolems, and Kiselyov's
  level generalization scheme, etc., etc.

* Just as importantly, existentials are a data/positive/inductive type, whereas
  records are a codata/negative/coinductive type. They are defined by their
  projections, and constructed by providing a response to each projection.

  Using field projections to extract fields of the existential is a similar
  muddle as using algebraic data types to encode records in Haskell.

Thus, here is my resolution on the matter:
* Existential quantification and record types are distinct constructs, and
  should not be conflated in the IR.
* "records with type fields" and "multi-argument pack/unpack" should be
  regarded as merely syntactic sugar for the obvious existential and record
  operations.

  For example, `unpack cl as <aa, envp, fn> -> fn(envp, args...)` is syntactic
  sugar for:
  ```
  let <aa, p : {envp : aa, code : code(aa, argTy...)}> = unpack cl in
  let envp = p.envp in
  let fn = p.code in
  fn(envp, args...)
  ```


